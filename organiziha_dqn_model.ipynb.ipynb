{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "m-tnOy2zRaLC",
        "outputId": "afbcf255-804a-42a2-b20d-7cd38fdda786"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Suggested Action:\n",
            "Not Notify\n",
            "Received Reward: 12.0\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# Define the DQN model\n",
        "class DQNModel(tf.keras.Model):\n",
        "    def __init__(self, num_actions):\n",
        "        super(DQNModel, self).__init__()\n",
        "        self.dense1 = layers.Dense(32, activation='relu')\n",
        "        self.dense2 = layers.Dense(32, activation='relu')\n",
        "        self.output_layer = layers.Dense(num_actions, activation='linear')\n",
        "\n",
        "    def call(self, state):\n",
        "        x = self.dense1(state)\n",
        "        x = self.dense2(x)\n",
        "        return self.output_layer(x)\n",
        "\n",
        "# Initialize the DQN model\n",
        "model = DQNModel(num_actions=2)  # 2 actions: Notify or Not Notify\n",
        "\n",
        "# Define DQN parameters\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "huber_loss = tf.keras.losses.Huber()\n",
        "\n",
        "# Manual simulation loop\n",
        "num_episodes = 1000\n",
        "\n",
        "for episode in range(num_episodes):\n",
        "    # Manual input for state values\n",
        "    noise = float(input(\"Enter noise level (0-1): \"))\n",
        "    deadline = float(input(\"Enter deadline (hours): \"))\n",
        "    task_priority = float(input(\"Enter task priority (0-1): \"))\n",
        "    time_of_day = float(input(\"Enter time of the day (0-24): \"))\n",
        "    task_duration = float(input(\"Enter task duration (hours): \"))\n",
        "    device_usage = float(input(\"Enter device usage (0-1): \"))\n",
        "\n",
        "    # Concatenate state variables\n",
        "    state = np.array([noise, deadline, task_priority, time_of_day, task_duration, device_usage])\n",
        "\n",
        "    # Reshape state for model input\n",
        "    state = np.reshape(state, (1, -1))\n",
        "\n",
        "    # Get the model's suggested action (0: Notify, 1: Not Notify)\n",
        "    q_values = model(state)\n",
        "    suggested_action = np.argmax(q_values.numpy())\n",
        "\n",
        "    print(\"Suggested Action:\")\n",
        "    if suggested_action == 0:\n",
        "        print(\"Notify\")\n",
        "    else:\n",
        "        print(\"Not Notify\")\n",
        "\n",
        "    # Manual input for reward\n",
        "    reward = float(input(\"Enter reward for the suggested action: \"))\n",
        "\n",
        "    # Train the model based on the provided reward\n",
        "    with tf.GradientTape() as tape:\n",
        "        q_values = model(state)\n",
        "        q_action = tf.reduce_sum(tf.one_hot(suggested_action, 2) * q_values, axis=1)\n",
        "        loss = huber_loss(reward, q_action)\n",
        "\n",
        "    grads = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "    print(\"Received Reward:\", reward)\n",
        "\n",
        "# Optionally, save the model weights\n",
        "# model.save_weights(\"dqn_model_weights.h5\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}